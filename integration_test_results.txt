============================= test session starts ==============================
collecting ... collected 170 items

tests/integration/test_backup_integration.py::test_backup_creates_real_docker_volume_backup SKIPPED [  0%]
tests/integration/test_backup_integration.py::test_backup_with_stack_running_performs_live_backup SKIPPED [  1%]
tests/integration/test_backup_integration.py::test_backup_with_stack_stopped_performs_offline_backup SKIPPED [  1%]
tests/integration/test_backup_integration.py::test_backup_preserves_large_volume_data SKIPPED [  2%]
tests/integration/test_backup_integration.py::test_backup_validates_manifest_structure PASSED [  2%]
tests/integration/test_backup_integration.py::test_backup_validation_detects_corrupted_manifest PASSED [  3%]
tests/integration/test_backup_integration.py::test_backup_validation_detects_missing_components PASSED [  4%]
tests/integration/test_backup_integration.py::test_backup_validation_cross_platform_compatibility PASSED [  4%]
tests/integration/test_backup_integration.py::test_backup_handles_insufficient_disk_space PASSED [  5%]
tests/integration/test_backup_integration.py::test_backup_handles_permission_denied_scenarios PASSED [  5%]
tests/integration/test_backup_integration.py::test_backup_handles_docker_service_interruption SKIPPED [  6%]
tests/integration/test_backup_integration.py::test_backup_handles_corrupted_source_data PASSED [  7%]
tests/integration/test_backup_integration.py::test_backup_apple_silicon_native_ollama_handling SKIPPED [  7%]
tests/integration/test_backup_integration.py::test_backup_docker_ollama_handling SKIPPED [  8%]
tests/integration/test_backup_integration.py::test_backup_performance_with_concurrent_operations SKIPPED [  8%]
tests/integration/test_backup_integration.py::test_backup_handles_multiple_concurrent_backups SKIPPED [  9%]
tests/integration/test_backup_integration.py::test_backup_after_update_operation SKIPPED [ 10%]
tests/integration/test_backup_integration.py::test_backup_with_fresh_installation PASSED [ 10%]
tests/integration/test_backup_integration.py::test_backup_error_recovery_and_cleanup SKIPPED [ 11%]
tests/integration/test_backup_integration.py::test_backup_output_format_consistency PASSED [ 11%]
tests/integration/test_backup_integration.py::test_backup_help_accessibility PASSED [ 12%]
tests/integration/test_install_integration.py::test_install_command_fresh_system_creates_config_files PASSED [ 12%]
tests/integration/test_install_integration.py::test_install_command_generates_unique_secure_keys PASSED [ 13%]
tests/integration/test_install_integration.py::test_install_command_creates_platform_specific_configurations PASSED [ 14%]
tests/integration/test_install_integration.py::test_install_command_runs_environment_validation PASSED [ 14%]
tests/integration/test_install_integration.py::test_install_over_existing_configuration_user_confirms PASSED [ 15%]
tests/integration/test_install_integration.py::test_install_over_existing_configuration_user_declines PASSED [ 15%]
tests/integration/test_install_integration.py::test_install_with_force_flag_overwrites_without_prompting PASSED [ 16%]
tests/integration/test_install_integration.py::test_install_partial_existing_configuration PASSED [ 17%]
tests/integration/test_install_integration.py::test_install_existing_directory_no_config_files PASSED [ 17%]
tests/integration/test_install_integration.py::test_install_command_help_accessibility PASSED [ 18%]
tests/integration/test_install_integration.py::test_install_enables_other_commands SKIPPED [ 18%]
tests/integration/test_install_integration.py::test_install_cross_platform_compatibility PASSED [ 19%]
tests/integration/test_install_integration.py::test_install_filesystem_permissions_verification PASSED [ 20%]
tests/integration/test_install_integration.py::test_install_configuration_file_format_validation PASSED [ 20%]
tests/integration/test_install_integration.py::test_install_error_message_quality FAILED [ 21%]
tests/integration/test_install_integration.py::test_install_idempotent_multiple_runs PASSED [ 21%]
tests/integration/test_install_integration.py::test_install_preserves_existing_non_config_files PASSED [ 22%]
tests/integration/test_install_integration.py::test_install_command_exit_codes PASSED [ 22%]
tests/integration/test_install_integration.py::test_install_command_output_format_consistency FAILED [ 23%]
tests/integration/test_install_integration.py::test_install_integration_with_stack_workflow SKIPPED [ 24%]
tests/integration/test_install_integration.py::test_install_without_docker_daemon PASSED [ 24%]
tests/integration/test_lifecycle_integration.py::test_start_and_stop_lifecycle SKIPPED [ 25%]
tests/integration/test_lifecycle_integration.py::test_start_command_without_docker FAILED [ 25%]
tests/integration/test_lifecycle_integration.py::test_start_when_already_running_is_idempotent SKIPPED [ 26%]
tests/integration/test_lifecycle_integration.py::test_stop_when_already_stopped_is_idempotent FAILED [ 27%]
tests/integration/test_lifecycle_integration.py::test_restart_recreates_services SKIPPED [ 27%]
tests/integration/test_lifecycle_integration.py::test_restart_without_docker FAILED [ 28%]
tests/integration/test_lifecycle_integration.py::test_restart_with_update_pulls_images SKIPPED [ 28%]
tests/integration/test_lifecycle_integration.py::test_start_with_update_pulls_images SKIPPED [ 29%]
tests/integration/test_lifecycle_integration.py::test_native_ollama_service_lifecycle SKIPPED [ 30%]
tests/integration/test_lifecycle_integration.py::test_docker_ollama_service_lifecycle SKIPPED [ 30%]
tests/integration/test_lifecycle_integration.py::test_service_health_after_start SKIPPED [ 31%]
tests/integration/test_lifecycle_integration.py::test_apple_silicon_without_ollama_installed SKIPPED [ 31%]
tests/integration/test_lifecycle_integration.py::test_status_command_reflects_actual_state SKIPPED [ 32%]
tests/integration/test_lifecycle_integration.py::test_check_command_validates_environment FAILED [ 32%]
tests/integration/test_lifecycle_integration.py::test_logs_command_accesses_actual_logs SKIPPED [ 33%]
tests/integration/test_lifecycle_integration.py::test_logs_command_with_follow_option SKIPPED [ 34%]
tests/integration/test_migrate_integration.py::test_migrate_detects_current_version SKIPPED [ 34%]
tests/integration/test_migrate_integration.py::test_migrate_dry_run_shows_migration_plan SKIPPED [ 35%]
tests/integration/test_migrate_integration.py::test_migrate_with_stack_stopped_performs_offline_migration SKIPPED [ 35%]
tests/integration/test_migrate_integration.py::test_migrate_with_stack_running_stops_migrates_restarts SKIPPED [ 36%]
tests/integration/test_migrate_integration.py::test_migrate_preserves_service_data_integrity SKIPPED [ 37%]
tests/integration/test_migrate_integration.py::test_migrate_target_version_validation PASSED [ 37%]
tests/integration/test_migrate_integration.py::test_migrate_same_version_handling PASSED [ 38%]
tests/integration/test_migrate_integration.py::test_migrate_validates_migration_prerequisites FAILED [ 38%]
tests/integration/test_migrate_integration.py::test_migrate_creates_backup_before_migration FAILED [ 39%]
tests/integration/test_migrate_integration.py::test_migrate_handles_docker_compatibility_checks SKIPPED [ 40%]
tests/integration/test_migrate_integration.py::test_migrate_cross_platform_compatibility_validation FAILED [ 40%]
tests/integration/test_migrate_integration.py::test_migrate_handles_insufficient_disk_space FAILED [ 41%]
tests/integration/test_migrate_integration.py::test_migrate_handles_docker_service_interruption SKIPPED [ 41%]
tests/integration/test_migrate_integration.py::test_migrate_rollback_on_failure PASSED [ 42%]
tests/integration/test_migrate_integration.py::test_migrate_handles_corrupted_configuration FAILED [ 42%]
tests/integration/test_migrate_integration.py::test_migrate_apple_silicon_native_ollama_handling SKIPPED [ 43%]
tests/integration/test_migrate_integration.py::test_migrate_docker_ollama_handling SKIPPED [ 44%]
tests/integration/test_migrate_integration.py::test_migrate_platform_detection_accuracy FAILED [ 44%]
tests/integration/test_migrate_integration.py::test_migrate_performance_timing SKIPPED [ 45%]
tests/integration/test_migrate_integration.py::test_migrate_system_resource_usage SKIPPED [ 45%]
tests/integration/test_migrate_integration.py::test_migrate_with_large_volume_data SKIPPED [ 46%]
tests/integration/test_migrate_integration.py::test_migrate_after_backup_operation SKIPPED [ 47%]
tests/integration/test_migrate_integration.py::test_migrate_followed_by_restore_workflow SKIPPED [ 47%]
tests/integration/test_migrate_integration.py::test_migrate_after_fresh_installation FAILED [ 48%]
tests/integration/test_migrate_integration.py::test_migrate_cleanup_on_failure FAILED [ 48%]
tests/integration/test_migrate_integration.py::test_migrate_system_consistency_after_interruption SKIPPED [ 49%]
tests/integration/test_migrate_integration.py::test_migrate_output_format_consistency FAILED [ 50%]
tests/integration/test_migrate_integration.py::test_migrate_help_accessibility PASSED [ 50%]
tests/integration/test_restore_integration.py::test_restore_performs_actual_volume_restoration SKIPPED [ 51%]
tests/integration/test_restore_integration.py::test_restore_validates_backup_before_restoration SKIPPED [ 51%]
tests/integration/test_restore_integration.py::test_restore_with_validate_only_flag FAILED [ 52%]
tests/integration/test_restore_integration.py::test_restore_with_force_flag_overrides_running_stack SKIPPED [ 52%]
tests/integration/test_restore_integration.py::test_restore_without_force_prompts_when_stack_running SKIPPED [ 53%]
tests/integration/test_restore_integration.py::test_restore_user_declines_when_stack_running SKIPPED [ 54%]
tests/integration/test_restore_integration.py::test_restore_detects_corrupted_backup PASSED [ 54%]
tests/integration/test_restore_integration.py::test_restore_detects_incomplete_backup PASSED [ 55%]
tests/integration/test_restore_integration.py::test_restore_handles_nonexistent_backup_path PASSED [ 55%]
tests/integration/test_restore_integration.py::test_restore_handles_permission_denied_scenarios FAILED [ 56%]
tests/integration/test_restore_integration.py::test_restore_handles_docker_service_interruption SKIPPED [ 57%]
tests/integration/test_restore_integration.py::test_restore_handles_insufficient_disk_space PASSED [ 57%]
tests/integration/test_restore_integration.py::test_restore_cross_platform_backup_compatibility FAILED [ 58%]
tests/integration/test_restore_integration.py::test_restore_apple_silicon_native_ollama_handling SKIPPED [ 58%]
tests/integration/test_restore_integration.py::test_restore_docker_ollama_handling SKIPPED [ 59%]
tests/integration/test_restore_integration.py::test_restore_preserves_configuration_files FAILED [ 60%]
tests/integration/test_restore_integration.py::test_restore_handles_configuration_conflicts FAILED [ 60%]
tests/integration/test_restore_integration.py::test_restore_performance_with_large_backup SKIPPED [ 61%]
tests/integration/test_restore_integration.py::test_restore_system_resource_usage SKIPPED [ 61%]
tests/integration/test_restore_integration.py::test_restore_after_complete_uninstall SKIPPED [ 62%]
tests/integration/test_restore_integration.py::test_restore_followed_by_start_workflow SKIPPED [ 62%]
tests/integration/test_restore_integration.py::test_restore_interruption_recovery SKIPPED [ 63%]
tests/integration/test_restore_integration.py::test_restore_cleanup_on_failure PASSED [ 64%]
tests/integration/test_restore_integration.py::test_restore_output_format_consistency FAILED [ 64%]
tests/integration/test_restore_integration.py::test_restore_help_accessibility PASSED [ 65%]
tests/integration/test_uninstall_integration.py::test_uninstall_basic_removes_docker_resources_preserves_data SKIPPED [ 65%]
tests/integration/test_uninstall_integration.py::test_uninstall_remove_volumes_actually_removes_data SKIPPED [ 66%]
tests/integration/test_uninstall_integration.py::test_uninstall_remove_config_actually_removes_filesystem_config SKIPPED [ 67%]
tests/integration/test_uninstall_integration.py::test_uninstall_all_flag_removes_everything SKIPPED [ 67%]
tests/integration/test_uninstall_integration.py::test_uninstall_short_form_all_flag_equivalent SKIPPED [ 68%]
tests/integration/test_uninstall_integration.py::test_uninstall_force_flag_handles_stuck_containers SKIPPED [ 68%]
tests/integration/test_uninstall_integration.py::test_uninstall_complex_flag_combinations SKIPPED [ 69%]
tests/integration/test_uninstall_integration.py::test_uninstall_stops_native_ollama_on_apple_silicon SKIPPED [ 70%]
tests/integration/test_uninstall_integration.py::test_uninstall_removes_docker_ollama_on_other_platforms SKIPPED [ 70%]
tests/integration/test_uninstall_integration.py::test_uninstall_when_stack_not_running SKIPPED [ 71%]
tests/integration/test_uninstall_integration.py::test_uninstall_removes_docker_images SKIPPED [ 71%]
tests/integration/test_uninstall_integration.py::test_uninstall_removes_docker_networks SKIPPED [ 72%]
tests/integration/test_uninstall_integration.py::test_uninstall_idempotent_multiple_runs SKIPPED [ 72%]
tests/integration/test_uninstall_integration.py::test_uninstall_without_docker_daemon FAILED [ 73%]
tests/integration/test_uninstall_integration.py::test_uninstall_preserves_non_stack_docker_resources SKIPPED [ 74%]
tests/integration/test_uninstall_integration.py::test_uninstall_handles_partial_resource_cleanup SKIPPED [ 74%]
tests/integration/test_uninstall_integration.py::test_uninstall_config_removal_filesystem_verification SKIPPED [ 75%]
tests/integration/test_uninstall_integration.py::test_uninstall_volume_removal_data_loss_verification SKIPPED [ 75%]
tests/integration/test_uninstall_integration.py::test_uninstall_complete_system_state_verification SKIPPED [ 76%]
tests/integration/test_uninstall_integration.py::test_uninstall_command_help_accessibility SKIPPED [ 77%]
tests/integration/test_uninstall_integration.py::test_uninstall_error_message_quality PASSED [ 77%]
tests/integration/test_uninstall_integration.py::test_uninstall_exit_codes_consistency SKIPPED [ 78%]
tests/integration/test_update_integration.py::test_update_command_when_stack_stopped SKIPPED [ 78%]
tests/integration/test_update_integration.py::test_update_command_services_only_flag SKIPPED [ 79%]
tests/integration/test_update_integration.py::test_update_command_extensions_only_flag SKIPPED [ 80%]
tests/integration/test_update_integration.py::test_update_command_conflicting_flags SKIPPED [ 80%]
tests/integration/test_update_integration.py::test_update_command_without_docker FAILED [ 81%]
tests/integration/test_update_integration.py::test_update_with_running_stack_user_confirms SKIPPED [ 81%]
tests/integration/test_update_integration.py::test_update_with_running_stack_user_declines SKIPPED [ 82%]
tests/integration/test_update_integration.py::test_start_with_update_integration SKIPPED [ 82%]
tests/integration/test_update_integration.py::test_restart_with_update_integration SKIPPED [ 83%]
tests/integration/test_update_integration.py::test_update_command_preserves_service_health SKIPPED [ 84%]
tests/integration/test_update_integration.py::test_update_command_idempotent SKIPPED [ 84%]
tests/integration/test_update_integration.py::test_update_command_help_accessibility SKIPPED [ 85%]
tests/integration/test_update_integration.py::test_update_with_network_interruption SKIPPED [ 85%]
tests/integration/test_update_integration.py::test_update_maintains_container_state_consistency SKIPPED [ 86%]
tests/integration/test_update_integration.py::test_update_services_only_excludes_extensions SKIPPED [ 87%]
tests/integration/test_update_integration.py::test_update_with_partial_service_failures SKIPPED [ 87%]
tests/integration/test_update_integration.py::test_update_stop_failure_handling SKIPPED [ 88%]
tests/integration/test_update_integration.py::test_update_restart_failure_handling SKIPPED [ 88%]
tests/integration/test_update_integration.py::test_concurrent_update_operations SKIPPED [ 89%]
tests/integration/test_update_integration.py::test_update_with_config_changes SKIPPED [ 90%]
tests/integration/test_update_integration.py::test_update_preserves_running_service_data SKIPPED [ 90%]
tests/integration/test_update_integration.py::test_update_command_resource_cleanup SKIPPED [ 91%]
tests/integration/test_update_integration.py::test_update_mixed_service_types_coordination SKIPPED [ 91%]
tests/integration/test_update_integration.py::test_update_performance_under_load SKIPPED [ 92%]
tests/integration/test_update_integration.py::test_update_stack_state_consistency_across_operations SKIPPED [ 92%]
tests/integration/test_update_integration.py::test_update_error_message_quality FAILED [ 93%]
tests/integration/test_workflow_integration.py::test_complete_stack_lifecycle_workflow SKIPPED [ 94%]
tests/integration/test_workflow_integration.py::test_backup_restore_migration_workflow SKIPPED [ 94%]
tests/integration/test_workflow_integration.py::test_disaster_recovery_workflow SKIPPED [ 95%]
tests/integration/test_workflow_integration.py::test_development_workflow_with_restarts SKIPPED [ 95%]
tests/integration/test_workflow_integration.py::test_error_recovery_across_commands SKIPPED [ 96%]
tests/integration/test_workflow_integration.py::test_concurrent_operation_handling SKIPPED [ 97%]
tests/integration/test_workflow_integration.py::test_performance_under_load_workflow SKIPPED [ 97%]
tests/integration/test_workflow_integration.py::test_long_running_stability_workflow SKIPPED [ 98%]
tests/integration/test_workflow_integration.py::test_configuration_persistence_across_operations PASSED [ 98%]
tests/integration/test_workflow_integration.py::test_user_friendly_error_messages_workflow FAILED [ 99%]
tests/integration/test_workflow_integration.py::test_help_accessibility_workflow PASSED [100%]

=================================== FAILURES ===================================
______________________ test_install_error_message_quality ______________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_install_error_message_quality(runner):
        """
        Verifies that install command provides high-quality error messages.
        """
        # Test help accessibility
        help_result = runner.invoke(app, ["install", "--help"])
        assert help_result.exit_code == 0
        assert "install" in help_result.stdout.lower()
    
        # Test with invalid arguments (if any)
        invalid_result = runner.invoke(app, ["install", "--invalid-flag"])
        assert invalid_result.exit_code != 0
    
        # Should have user-friendly error message
>       assert "invalid" in invalid_result.stdout.lower() or "unknown" in invalid_result.stdout.lower()
E       assert ('invalid' in "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n" or 'unknown' in "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n")
E        +  where "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n" = <built-in method lower of str object at 0x7d8780628570>()
E        +    where <built-in method lower of str object at 0x7d8780628570> = "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n".lower
E        +      where "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n" = <Result SystemExit(2)>.stdout
E        +  and   "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n" = <built-in method lower of str object at 0x7d87806288f0>()
E        +    where <built-in method lower of str object at 0x7d87806288f0> = "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n".lower
E        +      where "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n" = <Result SystemExit(2)>.stdout

tests/integration/test_install_integration.py:493: AssertionError
________________ test_install_command_output_format_consistency ________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
clean_config_dir = '/home/ubuntu/.ollama-stack'

    @pytest.mark.integration
    def test_install_command_output_format_consistency(runner, clean_config_dir):
        """
        Verifies that install command output is consistent and well-formatted.
        """
        config_dir = clean_config_dir
    
        # Run install command
        result = runner.invoke(app, ["install", "--force"])
        assert result.exit_code == 0
    
        # Output should be well-formatted
        output_lines = result.stdout.strip().split('\n')
    
        # Should have meaningful output
        assert len(output_lines) > 0
    
        # Should contain success indication
        output_text = result.stdout.lower()
        assert any(keyword in output_text for keyword in [
            "installation", "completed", "success", "created", "configured"
        ])
    
        # Should not contain error messages
>       assert "error" not in output_text
E       assert 'error' not in '[02:46:41] creating default configuration files in /home/ubuntu/.ollama-stack   \n           could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: (\'connection aborted.\', filenotfounderror(2, \'no such file  \n           or directory\'))                                                      \n           starting fresh stack installation...                                 \n           initializing fresh stack configuration...                            \n           configuration directory already exists: /home/ubuntu/.ollama-stack   \n           created configuration directory: /home/ubuntu/.ollama-stack          \n           created default configuration files                                  \n           configuration files created successfully!                            \n           running environment validation checks...                             \n           validating environment...                                            \n           failed to create configuration: \'nonetype\' object has no attribute   \n           \'ping\'                                                               \n           â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n           â”‚ /workspace/ollama_stack_cli/stack_manager.py:189 in install_stack â”‚\n           â”‚                                                                   â”‚\n           â”‚    186 â”‚   â”‚   â”‚   log.info("running environment validation check â”‚\n           â”‚    187 â”‚   â”‚   â”‚   log.info("validating environment...")          â”‚\n           â”‚    188 â”‚   â”‚   â”‚                                                  â”‚\n           â”‚ â±  189 â”‚   â”‚   â”‚   check_report = self.run_environment_checks(fix â”‚\n           â”‚    190 â”‚   â”‚   â”‚                                                  â”‚\n           â”‚    191 â”‚   â”‚   â”‚   # check if all critical checks passed          â”‚\n           â”‚    192 â”‚   â”‚   â”‚   failed_checks = [check for check in check_repo â”‚\n           â”‚                                                                   â”‚\n           â”‚ /workspace/ollama_stack_cli/stack_manager.py:104 in               â”‚\n           â”‚ run_environment_checks                                            â”‚\n           â”‚                                                                   â”‚\n           â”‚    101 â”‚   â”‚   log.debug("running comprehensive environment check â”‚\n           â”‚    102 â”‚   â”‚                                                      â”‚\n           â”‚    103 â”‚   â”‚   # delegate environment checks to docker client     â”‚\n           â”‚ â±  104 â”‚   â”‚   report = self.docker_client.run_environment_checks â”‚\n           â”‚        platform=self.platform)                                    â”‚\n           â”‚    105 â”‚   â”‚                                                      â”‚\n           â”‚    106 â”‚   â”‚   # add platform-specific checks for native services â”‚\n           â”‚    107 â”‚   â”‚   if self.platform == "apple":                       â”‚\n           â”‚                                                                   â”‚\n           â”‚ /workspace/ollama_stack_cli/docker_client.py:256 in               â”‚\n           â”‚ run_environment_checks                                            â”‚\n           â”‚                                                                   â”‚\n           â”‚   253 â”‚   â”‚                                                       â”‚\n           â”‚   254 â”‚   â”‚   # 1. docker daemon check                            â”‚\n           â”‚   255 â”‚   â”‚   try:                                                â”‚\n           â”‚ â± 256 â”‚   â”‚   â”‚   self.client.ping()                              â”‚\n           â”‚   257 â”‚   â”‚   â”‚   log.debug("docker daemon is running and accessi â”‚\n           â”‚   258 â”‚   â”‚   â”‚   checks.append(environmentcheck(                 â”‚\n           â”‚   259 â”‚   â”‚   â”‚   â”‚   name="docker daemon running",               â”‚\n           â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n           attributeerror: \'nonetype\' object has no attribute \'ping\'            \nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ error: failed to create configuration: \'nonetype\' object has no attribute    â”‚\nâ”‚ \'ping\'                                                                       â”‚\nâ”‚                                                                              â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n           installation failed                                                  \n'
E         
E         'error' is contained here:
E           [02:46:41] creating default configuration files in /home/ubuntu/.ollama-stack   
E                      could not get docker info to check for nvidia runtime.               
E                      defaulting to cpu platform.                                          
E                      failed to initialize docker client: error while fetching server api  
E         ?                                                +++++
E                      version: ('connection aborted.', filenotfounderror(2, 'no such file  
E                      or directory'))                                                      
E                      starting fresh stack installation...                                 
E                      initializing fresh stack configuration...                            
E                      configuration directory already exists: /home/ubuntu/.ollama-stack   
E                      created configuration directory: /home/ubuntu/.ollama-stack          
E                      created default configuration files                                  
E                      configuration files created successfully!                            
E                      running environment validation checks...                             
E                      validating environment...                                            
E                      failed to create configuration: 'nonetype' object has no attribute   
E                      'ping'                                                               
E                      â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
E                      â”‚ /workspace/ollama_stack_cli/stack_manager.py:189 in install_stack â”‚
E                      â”‚                                                                   â”‚
E                      â”‚    186 â”‚   â”‚   â”‚   log.info("running environment validation check â”‚
E                      â”‚    187 â”‚   â”‚   â”‚   log.info("validating environment...")          â”‚
E                      â”‚    188 â”‚   â”‚   â”‚                                                  â”‚
E                      â”‚ â±  189 â”‚   â”‚   â”‚   check_report = self.run_environment_checks(fix â”‚
E                      â”‚    190 â”‚   â”‚   â”‚                                                  â”‚
E                      â”‚    191 â”‚   â”‚   â”‚   # check if all critical checks passed          â”‚
E                      â”‚    192 â”‚   â”‚   â”‚   failed_checks = [check for check in check_repo â”‚
E                      â”‚                                                                   â”‚
E                      â”‚ /workspace/ollama_stack_cli/stack_manager.py:104 in               â”‚
E                      â”‚ run_environment_checks                                            â”‚
E                      â”‚                                                                   â”‚
E                      â”‚    101 â”‚   â”‚   log.debug("running comprehensive environment check â”‚
E                      â”‚    102 â”‚   â”‚                                                      â”‚
E                      â”‚    103 â”‚   â”‚   # delegate environment checks to docker client     â”‚
E                      â”‚ â±  104 â”‚   â”‚   report = self.docker_client.run_environment_checks â”‚
E                      â”‚        platform=self.platform)                                    â”‚
E                      â”‚    105 â”‚   â”‚                                                      â”‚
E                      â”‚    106 â”‚   â”‚   # add platform-specific checks for native services â”‚
E                      â”‚    107 â”‚   â”‚   if self.platform == "apple":                       â”‚
E                      â”‚                                                                   â”‚
E                      â”‚ /workspace/ollama_stack_cli/docker_client.py:256 in               â”‚
E                      â”‚ run_environment_checks                                            â”‚
E                      â”‚                                                                   â”‚
E                      â”‚   253 â”‚   â”‚                                                       â”‚
E                      â”‚   254 â”‚   â”‚   # 1. docker daemon check                            â”‚
E                      â”‚   255 â”‚   â”‚   try:                                                â”‚
E                      â”‚ â± 256 â”‚   â”‚   â”‚   self.client.ping()                              â”‚
E                      â”‚   257 â”‚   â”‚   â”‚   log.debug("docker daemon is running and accessi â”‚
E                      â”‚   258 â”‚   â”‚   â”‚   checks.append(environmentcheck(                 â”‚
E                      â”‚   259 â”‚   â”‚   â”‚   â”‚   name="docker daemon running",               â”‚
E                      â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
E                      attributeerror: 'nonetype' object has no attribute 'ping'            
E           â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
E           â”‚ error: failed to create configuration: 'nonetype' object has no attribute    â”‚
E           â”‚ 'ping'                                                                       â”‚
E           â”‚                                                                              â”‚
E           â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
E                      installation failed

tests/integration/test_install_integration.py:621: AssertionError
______________________ test_start_command_without_docker _______________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_start_command_without_docker(runner):
        """
        Verifies that when Docker daemon is not running, the start command
        fails gracefully with a clean, user-friendly error message.
    
        This tests the actual error handling users would experience.
        """
        if is_docker_available():
            pytest.skip("Docker is available - testing Docker unavailable scenario")
    
        result = runner.invoke(app, ["start"])
    
        # Should exit with error code
        assert result.exit_code == 1
    
        # Should contain helpful error message about Docker daemon (not messy traceback)
        output_lower = result.stdout.lower()
>       assert any(keyword in output_lower for keyword in [
            "docker daemon", "daemon is not running", "docker desktop"
        ]), f"Expected Docker daemon error message, got: {result.stdout}"
E       AssertionError: Expected Docker daemon error message, got: [02:46:43] Could not get Docker info to check for NVIDIA runtime.               
E                    Defaulting to CPU platform.                                          
E                    Failed to initialize Docker client: Error while fetching server API  
E                    version: ('Connection aborted.', FileNotFoundError(2, 'No such file  
E                    or directory'))                                                      
E         â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Security Warning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
E         â”‚ âš ï¸  Warning: Using default configuration with placeholder security keys   â”‚
E         â”‚                                                                          â”‚
E         â”‚ ğŸ“‹ Run 'ollama-stack install' to generate a unique, secure configuration â”‚
E         â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
E         
E       assert False
E        +  where False = any(<generator object test_start_command_without_docker.<locals>.<genexpr> at 0x7d8780a8dd80>)

tests/integration/test_lifecycle_integration.py:65: AssertionError
_________________ test_stop_when_already_stopped_is_idempotent _________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_stop_when_already_stopped_is_idempotent(runner):
        """
        Verifies that running 'stop' on an already stopped stack has no
        negative side effects and exits gracefully.
        """
        # The fixture ensures the stack is already stopped.
        result = runner.invoke(app, ["stop"])
>       assert result.exit_code == 0
E       AssertionError: assert 1 == 0
E        +  where 1 = <Result FileNotFoundError(2, 'No such file or directory')>.exit_code

tests/integration/test_lifecycle_integration.py:105: AssertionError
_________________________ test_restart_without_docker __________________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_restart_without_docker(runner):
        """
        Verifies that when Docker daemon is not running, the restart command
        fails gracefully with a clean, user-friendly error message.
        """
        if is_docker_available():
            pytest.skip("Docker is available - testing Docker unavailable scenario")
    
        result = runner.invoke(app, ["restart"])
    
        # Should exit with error code
        assert result.exit_code == 1
    
        # Should contain helpful error message about Docker daemon (not messy traceback)
        output_lower = result.stdout.lower()
>       assert any(keyword in output_lower for keyword in [
            "docker daemon", "daemon is not running", "docker desktop"
        ]), f"Expected Docker daemon error message, got: {result.stdout}"
E       AssertionError: Expected Docker daemon error message, got: [02:46:45] Could not get Docker info to check for NVIDIA runtime.               
E                    Defaulting to CPU platform.                                          
E                    Failed to initialize Docker client: Error while fetching server API  
E                    version: ('Connection aborted.', FileNotFoundError(2, 'No such file  
E                    or directory'))                                                      
E                    Restarting Ollama Stack...                                           
E                    Stopping Docker-based services...                                    
E         
E       assert False
E        +  where False = any(<generator object test_restart_without_docker.<locals>.<genexpr> at 0x7d87806d9ff0>)

tests/integration/test_lifecycle_integration.py:155: AssertionError
___________________ test_check_command_validates_environment ___________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_check_command_validates_environment(runner):
        """
        Verifies the 'check' command provides accurate environment validation.
    
        This tests actual system state detection.
        """
        result = runner.invoke(app, ["check"])
    
        # Check command should exit with appropriate code based on actual state
        if is_docker_available():
            # If Docker daemon is running, check should succeed or show minor issues
            assert result.exit_code == 0
        else:
            # If Docker daemon is not running, check should fail (this is the correct behavior)
            assert result.exit_code != 0
    
        # Should always check Docker daemon
        assert "docker" in result.stdout.lower()
    
        # Should check port availability
>       assert "port" in result.stdout.lower()
E       assert 'port' in "[02:46:46] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n           running environment checks...                                        \n"
E        +  where "[02:46:46] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n           running environment checks...                                        \n" = <built-in method lower of str object at 0x1d7710c0>()
E        +    where <built-in method lower of str object at 0x1d7710c0> = "[02:46:46] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n           Running environment checks...                                        \n".lower
E        +      where "[02:46:46] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n           Running environment checks...                                        \n" = <Result AttributeError("'NoneType' object has no attribute 'ping'")>.stdout

tests/integration/test_lifecycle_integration.py:353: AssertionError
________________ test_migrate_validates_migration_prerequisites ________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_migrate_validates_migration_prerequisites(runner):
        """
        Verifies migrate validates system prerequisites before migration.
    
        Tests pre-migration validation checks.
        """
        # Ensure configuration exists
        install_result = runner.invoke(app, ["install", "--force"])
        assert install_result.exit_code == 0
    
        # Run migration (should validate prerequisites)
        result = runner.invoke(app, ["migrate", "--target-version", "0.3.0"])
>       assert result.exit_code in [0, 1]  # May succeed or fail based on environment
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       assert 2 in [0, 1]
E        +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:272: AssertionError
_________________ test_migrate_creates_backup_before_migration _________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
temp_backup_dir = PosixPath('/tmp/pytest-of-ubuntu/pytest-2/test_migrate_creates_backup_be0/backup')

    @pytest.mark.integration
    def test_migrate_creates_backup_before_migration(runner, temp_backup_dir):
        """
        Verifies migrate creates automatic backup before performing migration.
    
        Tests automatic backup creation for safety.
        """
        # Ensure configuration exists
        install_result = runner.invoke(app, ["install", "--force"])
        assert install_result.exit_code == 0
    
        # Perform migration (should create backup automatically)
        result = runner.invoke(app, ["migrate", "--target-version", "0.3.0"])
>       assert result.exit_code == 0
E       assert 2 == 0
E        +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:302: AssertionError
_____________ test_migrate_cross_platform_compatibility_validation _____________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_migrate_cross_platform_compatibility_validation(runner):
        """
        Verifies migrate validates cross-platform compatibility.
    
        Tests platform-specific migration validation.
        """
        # Ensure configuration exists
        install_result = runner.invoke(app, ["install", "--force"])
        assert install_result.exit_code == 0
    
        # Run migration dry-run to see platform validation
        result = runner.invoke(app, ["migrate", "--dry-run", "--target-version", "0.3.0"])
>       assert result.exit_code == 0
E       assert 2 == 0
E        +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:356: AssertionError
_________________ test_migrate_handles_insufficient_disk_space _________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_migrate_handles_insufficient_disk_space(runner):
        """
        Verifies migrate behavior when disk space is insufficient.
    
        Tests handling of storage limitations during migration.
        """
        # Ensure configuration exists
        install_result = runner.invoke(app, ["install", "--force"])
        assert install_result.exit_code == 0
    
        # Create large file to simulate disk space issues
        temp_dir = Path(tempfile.gettempdir()) / "ollama_stack_test"
        temp_dir.mkdir(exist_ok=True)
    
        large_file = simulate_disk_full_scenario(temp_dir)
    
        if large_file:  # Only run if we could create the large file
            try:
                # Attempt migration with limited space
                result = runner.invoke(app, ["migrate", "--target-version", "0.3.0"])
    
                # Should handle gracefully
>               assert result.exit_code in [0, 1]
E               assert 2 in [0, 1]
E                +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:397: AssertionError
_________________ test_migrate_handles_corrupted_configuration _________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_migrate_handles_corrupted_configuration(runner):
        """
        Verifies migrate handles corrupted configuration gracefully.
    
        Tests error handling for invalid source configuration.
        """
        # Create corrupted configuration
        config_dir = os.path.expanduser("~/.ollama-stack")
        os.makedirs(config_dir, exist_ok=True)
    
        config_file = os.path.join(config_dir, ".ollama-stack.json")
        with open(config_file, 'w') as f:
            f.write('{"invalid": "json"')  # Missing closing brace
    
        try:
            # Attempt migration with corrupted config
            result = runner.invoke(app, ["migrate", "--target-version", "0.3.0"])
>           assert result.exit_code == 1
E           assert 2 == 1
E            +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:505: AssertionError
___________________ test_migrate_platform_detection_accuracy ___________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_migrate_platform_detection_accuracy(runner):
        """
        Verifies migration accurately detects and reports platform information.
    
        Tests platform detection reliability.
        """
        # Ensure configuration exists
        install_result = runner.invoke(app, ["install", "--force"])
        assert install_result.exit_code == 0
    
        # Run migration dry-run to see platform detection
        result = runner.invoke(app, ["migrate", "--dry-run", "--target-version", "0.3.0"])
>       assert result.exit_code == 0
E       assert 2 == 0
E        +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:596: AssertionError
____________________ test_migrate_after_fresh_installation _____________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
clean_config_dir = '/home/ubuntu/.ollama-stack'

    @pytest.mark.integration
    def test_migrate_after_fresh_installation(runner, clean_config_dir):
        """
        Verifies migration works correctly after fresh installation.
    
        Tests install -> migrate workflow.
        """
        # Fresh installation
        install_result = runner.invoke(app, ["install", "--force"])
        assert install_result.exit_code == 0
    
        # Immediate migration after install
        result = runner.invoke(app, ["migrate", "--target-version", "0.3.0"])
>       assert result.exit_code == 0
E       assert 2 == 0
E        +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:789: AssertionError
_______________________ test_migrate_cleanup_on_failure ________________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_migrate_cleanup_on_failure(runner):
        """
        Verifies migration cleans up properly on failure.
    
        Tests that failed migrations don't leave partial state.
        """
        # Create minimal configuration that might cause migration issues
        config_dir = os.path.expanduser("~/.ollama-stack")
        os.makedirs(config_dir, exist_ok=True)
    
        config_file = os.path.join(config_dir, ".ollama-stack.json")
        with open(config_file, 'w') as f:
            json.dump({"minimal": "config"}, f)
    
        try:
            # Attempt migration (may fail due to incomplete config)
            result = runner.invoke(app, ["migrate", "--target-version", "0.3.0"])
    
            # Should handle gracefully
>           assert result.exit_code in [0, 1]
E           assert 2 in [0, 1]
E            +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:823: AssertionError
____________________ test_migrate_output_format_consistency ____________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_migrate_output_format_consistency(runner):
        """
        Verifies migrate command output is consistent and well-formatted.
    
        Tests user experience and output quality.
        """
        # Ensure configuration exists
        install_result = runner.invoke(app, ["install", "--force"])
        assert install_result.exit_code == 0
    
        # Perform migration
        result = runner.invoke(app, ["migrate", "--target-version", "0.3.0"])
>       assert result.exit_code == 0
E       assert 2 == 0
E        +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_migrate_integration.py:891: AssertionError
_____________________ test_restore_with_validate_only_flag _____________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
temp_backup_dir = PosixPath('/tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup')

    @pytest.mark.integration
    def test_restore_with_validate_only_flag(runner, temp_backup_dir):
        """
        Verifies restore --validate-only performs validation without restoration.
    
        Tests validation-only mode for backup verification.
        """
        # Create valid backup structure
        create_test_backup_structure(temp_backup_dir)
    
        # Run validation only
        result = runner.invoke(app, ["restore", "--validate-only", str(temp_backup_dir)])
        assert result.exit_code == 0
    
        # Should show validation results
        output_lower = result.stdout.lower()
        assert "validation" in output_lower
        assert "successful" in output_lower
    
        # Should NOT perform actual restore
>       assert "restoring" not in output_lower
E       assert 'restoring' not in "[02:47:01] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n           restoring from backup:                                               \n           /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup\n           validating backup integrity...                                       \n           starting stack restore process...                                    \n           validating backup manifest...                                        \n           backup manifest validation completed successfully                    \n           backup validation passed                                             \n           backup id: test-backup-12345                                         \n           created: 2024-01-01 00:00:00+00:00                                   \n           platform: default                                                    \n           validation-only mode - restore not performed                         \n           backup validation passed                                             \n           validation-only mode - restore not performed                         \nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ validation complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ âœ… backup validation successful                                              â”‚\nâ”‚                                                                              â”‚\nâ”‚ ğŸ“ backup:                                                                   â”‚\nâ”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚\nâ”‚ ğŸ” status: valid and ready for restore                                       â”‚\nâ”‚                                                                              â”‚\nâ”‚ ğŸ’¡ to restore this backup, run:                                              â”‚\nâ”‚    ollama-stack restore                                                      â”‚\nâ”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
E         
E         'restoring' is contained here:
E           [02:47:01] could not get docker info to check for nvidia runtime.               
E                      defaulting to cpu platform.                                          
E                      failed to initialize docker client: error while fetching server api  
E                      version: ('connection aborted.', filenotfounderror(2, 'no such file  
E                      or directory'))                                                      
E                      restoring from backup:                                               
E         ?            +++++++++
E                      /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup
E                      validating backup integrity...                                       
E                      starting stack restore process...                                    
E                      validating backup manifest...                                        
E                      backup manifest validation completed successfully                    
E                      backup validation passed                                             
E                      backup id: test-backup-12345                                         
E                      created: 2024-01-01 00:00:00+00:00                                   
E                      platform: default                                                    
E                      validation-only mode - restore not performed                         
E                      backup validation passed                                             
E                      validation-only mode - restore not performed                         
E           â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ validation complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
E           â”‚ âœ… backup validation successful                                              â”‚
E           â”‚                                                                              â”‚
E           â”‚ ğŸ“ backup:                                                                   â”‚
E           â”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚
E           â”‚ ğŸ” status: valid and ready for restore                                       â”‚
E           â”‚                                                                              â”‚
E           â”‚ ğŸ’¡ to restore this backup, run:                                              â”‚
E           â”‚    ollama-stack restore                                                      â”‚
E           â”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚
E           â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

tests/integration/test_restore_integration.py:127: AssertionError
_______________ test_restore_handles_permission_denied_scenarios _______________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
temp_backup_dir = PosixPath('/tmp/pytest-of-ubuntu/pytest-2/test_restore_handles_permissio0/backup')

    @pytest.mark.integration
    def test_restore_handles_permission_denied_scenarios(runner, temp_backup_dir):
        """
        Verifies restore handles permission restrictions gracefully.
    
        Tests handling of filesystem permission issues during restore.
        """
        # Create backup structure
        create_test_backup_structure(temp_backup_dir)
    
        # Remove read permissions from backup directory
        try:
            os.chmod(temp_backup_dir, 0o000)
    
            # Restore should fail gracefully
            result = runner.invoke(app, ["restore", str(temp_backup_dir)])
            assert result.exit_code == 1
    
            # Should show permission error
            output_lower = result.stdout.lower()
>           assert any(keyword in output_lower for keyword in [
                "permission", "denied", "access", "cannot read"
            ])
E           assert False
E            +  where False = any(<generator object test_restore_handles_permission_denied_scenarios.<locals>.<genexpr> at 0x7d87806da740>)

tests/integration/test_restore_integration.py:311: AssertionError
_______________ test_restore_cross_platform_backup_compatibility _______________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
temp_backup_dir = PosixPath('/tmp/pytest-of-ubuntu/pytest-2/test_restore_cross_platform_ba0/backup')

    @pytest.mark.integration
    def test_restore_cross_platform_backup_compatibility(runner, temp_backup_dir):
        """
        Verifies restore works with backups from different platforms.
    
        Tests cross-platform backup compatibility.
        """
        # Create backup structure
        create_test_backup_structure(temp_backup_dir)
    
        # Modify manifest to simulate different platform
        manifest_path = temp_backup_dir / "manifest.json"
>       with open(manifest_path, 'r') as f:
             ^^^^^^^^^^^^^^^^^^^^^^^^
E       FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pytest-of-ubuntu/pytest-2/test_restore_cross_platform_ba0/backup/manifest.json'

tests/integration/test_restore_integration.py:409: FileNotFoundError
__________________ test_restore_preserves_configuration_files __________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
temp_backup_dir = PosixPath('/tmp/pytest-of-ubuntu/pytest-2/test_restore_preserves_configu0/backup')
clean_config_dir = '/home/ubuntu/.ollama-stack'

    @pytest.mark.integration
    def test_restore_preserves_configuration_files(runner, temp_backup_dir, clean_config_dir):
        """
        Verifies restore correctly restores configuration files.
    
        Tests configuration restoration and validation.
        """
        # Create backup with configuration
        create_test_backup_structure(temp_backup_dir)
    
        # Restore should restore configuration
        result = runner.invoke(app, ["restore", str(temp_backup_dir)])
>       assert result.exit_code == 0
E       assert 1 == 0
E        +  where 1 = <Result SystemExit(1)>.exit_code

tests/integration/test_restore_integration.py:510: AssertionError
_________________ test_restore_handles_configuration_conflicts _________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
temp_backup_dir = PosixPath('/tmp/pytest-of-ubuntu/pytest-2/test_restore_handles_configura0/backup')
clean_config_dir = '/home/ubuntu/.ollama-stack'

    @pytest.mark.integration
    def test_restore_handles_configuration_conflicts(runner, temp_backup_dir, clean_config_dir):
        """
        Verifies restore handles existing configuration conflicts.
    
        Tests configuration conflict resolution during restore.
        """
        # Create existing configuration
        config_dir = clean_config_dir
        os.makedirs(config_dir, exist_ok=True)
    
        existing_config = {"docker_compose_file": "existing.yml"}
        with open(os.path.join(config_dir, ".ollama-stack.json"), 'w') as f:
            json.dump(existing_config, f)
    
        with open(os.path.join(config_dir, ".env"), 'w') as f:
            f.write("PROJECT_NAME=existing-stack\nWEBUI_SECRET_KEY=existing-key")
    
        # Create backup with different configuration
        create_test_backup_structure(temp_backup_dir)
    
        # Restore should handle configuration conflict
        result = runner.invoke(app, ["restore", str(temp_backup_dir)])
>       assert result.exit_code == 0
E       assert 1 == 0
E        +  where 1 = <Result SystemExit(1)>.exit_code

tests/integration/test_restore_integration.py:554: AssertionError
____________________ test_restore_output_format_consistency ____________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>
temp_backup_dir = PosixPath('/tmp/pytest-of-ubuntu/pytest-2/test_restore_output_format_con0/backup')

    @pytest.mark.integration
    def test_restore_output_format_consistency(runner, temp_backup_dir):
        """
        Verifies restore command output is consistent and well-formatted.
    
        Tests user experience and output quality.
        """
        # Create backup structure
        create_test_backup_structure(temp_backup_dir)
    
        # Perform restore
        result = runner.invoke(app, ["restore", str(temp_backup_dir)])
>       assert result.exit_code == 0
E       assert 1 == 0
E        +  where 1 = <Result SystemExit(1)>.exit_code

tests/integration/test_restore_integration.py:770: AssertionError
_____________________ test_uninstall_without_docker_daemon _____________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_uninstall_without_docker_daemon(runner):
        """
        Verifies that uninstall handles Docker unavailability gracefully.
        """
        if is_docker_available():
            pytest.skip("Docker is available - testing Docker unavailable scenario")
    
        result = runner.invoke(app, ["uninstall"])
    
        # Should handle Docker unavailability gracefully
        assert result.exit_code in [0, 1]  # May succeed or fail gracefully
    
        # Should not crash with technical errors
        output_lower = result.stdout.lower()
        assert "traceback" not in output_lower
        assert "exception" not in output_lower
    
        # Should provide helpful message
>       assert any(keyword in output_lower for keyword in [
            "docker daemon", "docker desktop", "not running", "unavailable"
        ])
E       assert False
E        +  where False = any(<generator object test_uninstall_without_docker_daemon.<locals>.<genexpr> at 0x7d8780a1d970>)

tests/integration/test_uninstall_integration.py:453: AssertionError
______________________ test_update_command_without_docker ______________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_update_command_without_docker(runner):
        """
        Verifies that update command handles Docker unavailability gracefully.
        """
        if is_docker_available():
            pytest.skip("Docker is available - testing Docker unavailable scenario")
    
        result = runner.invoke(app, ["update"])
    
        # Should exit with error code
        assert result.exit_code != 0
    
        # Should contain helpful error message about Docker
        output_lower = result.stdout.lower()
>       assert any(keyword in output_lower for keyword in [
            "docker daemon", "daemon is not running", "docker desktop"
        ])
E       assert False
E        +  where False = any(<generator object test_update_command_without_docker.<locals>.<genexpr> at 0x7d8780a3d080>)

tests/integration/test_update_integration.py:104: AssertionError
______________________ test_update_error_message_quality _______________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_update_error_message_quality(runner):
        """
        Verifies that update command provides high-quality error messages.
        """
        # Test with Docker unavailable
        if not is_docker_available():
            result = runner.invoke(app, ["update"])
            assert result.exit_code != 0
    
            # Should have user-friendly error message
            output_lower = result.stdout.lower()
>           assert any(keyword in output_lower for keyword in [
                "docker daemon", "docker desktop", "not running"
            ])
E           assert False
E            +  where False = any(<generator object test_update_error_message_quality.<locals>.<genexpr> at 0x7d878055d3c0>)

tests/integration/test_update_integration.py:659: AssertionError
__________________ test_user_friendly_error_messages_workflow __________________

runner = <typer.testing.CliRunner object at 0x7d8780aaf620>

    @pytest.mark.integration
    def test_user_friendly_error_messages_workflow(runner):
        """
        Verifies user-friendly error messages across command workflows.
    
        Tests that error messages are helpful and not technical.
        """
        # Test commands without installation
        commands_to_test = [
            ["start"],
            ["stop"],
            ["restart"],
            ["status"],
            ["backup", "--output", "/tmp/test_backup"],
            ["restore", "/nonexistent/backup"],
            ["migrate", "--target-version", "0.3.0"]
        ]
    
        for command in commands_to_test:
            result = runner.invoke(app, command)
    
            # Commands may fail without installation, but should fail gracefully
>           assert result.exit_code in [0, 1]
E           assert 2 in [0, 1]
E            +  where 2 = <Result SystemExit(2)>.exit_code

tests/integration/test_workflow_integration.py:550: AssertionError
=========================== short test summary info ============================
FAILED tests/integration/test_install_integration.py::test_install_error_message_quality - assert ('invalid' in "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n" or 'unknown' in "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n")
 +  where "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n" = <built-in method lower of str object at 0x7d8780628570>()
 +    where <built-in method lower of str object at 0x7d8780628570> = "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n".lower
 +      where "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n" = <Result SystemExit(2)>.stdout
 +  and   "[02:46:37] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n" = <built-in method lower of str object at 0x7d87806288f0>()
 +    where <built-in method lower of str object at 0x7d87806288f0> = "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n".lower
 +      where "[02:46:37] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n" = <Result SystemExit(2)>.stdout
FAILED tests/integration/test_install_integration.py::test_install_command_output_format_consistency - assert 'error' not in '[02:46:41] creating default configuration files in /home/ubuntu/.ollama-stack   \n           could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: (\'connection aborted.\', filenotfounderror(2, \'no such file  \n           or directory\'))                                                      \n           starting fresh stack installation...                                 \n           initializing fresh stack configuration...                            \n           configuration directory already exists: /home/ubuntu/.ollama-stack   \n           created configuration directory: /home/ubuntu/.ollama-stack          \n           created default configuration files                                  \n           configuration files created successfully!                            \n           running environment validation checks...                             \n           validating environment...                                            \n           failed to create configuration: \'nonetype\' object has no attribute   \n           \'ping\'                                                               \n           â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n           â”‚ /workspace/ollama_stack_cli/stack_manager.py:189 in install_stack â”‚\n           â”‚                                                                   â”‚\n           â”‚    186 â”‚   â”‚   â”‚   log.info("running environment validation check â”‚\n           â”‚    187 â”‚   â”‚   â”‚   log.info("validating environment...")          â”‚\n           â”‚    188 â”‚   â”‚   â”‚                                                  â”‚\n           â”‚ â±  189 â”‚   â”‚   â”‚   check_report = self.run_environment_checks(fix â”‚\n           â”‚    190 â”‚   â”‚   â”‚                                                  â”‚\n           â”‚    191 â”‚   â”‚   â”‚   # check if all critical checks passed          â”‚\n           â”‚    192 â”‚   â”‚   â”‚   failed_checks = [check for check in check_repo â”‚\n           â”‚                                                                   â”‚\n           â”‚ /workspace/ollama_stack_cli/stack_manager.py:104 in               â”‚\n           â”‚ run_environment_checks                                            â”‚\n           â”‚                                                                   â”‚\n           â”‚    101 â”‚   â”‚   log.debug("running comprehensive environment check â”‚\n           â”‚    102 â”‚   â”‚                                                      â”‚\n           â”‚    103 â”‚   â”‚   # delegate environment checks to docker client     â”‚\n           â”‚ â±  104 â”‚   â”‚   report = self.docker_client.run_environment_checks â”‚\n           â”‚        platform=self.platform)                                    â”‚\n           â”‚    105 â”‚   â”‚                                                      â”‚\n           â”‚    106 â”‚   â”‚   # add platform-specific checks for native services â”‚\n           â”‚    107 â”‚   â”‚   if self.platform == "apple":                       â”‚\n           â”‚                                                                   â”‚\n           â”‚ /workspace/ollama_stack_cli/docker_client.py:256 in               â”‚\n           â”‚ run_environment_checks                                            â”‚\n           â”‚                                                                   â”‚\n           â”‚   253 â”‚   â”‚                                                       â”‚\n           â”‚   254 â”‚   â”‚   # 1. docker daemon check                            â”‚\n           â”‚   255 â”‚   â”‚   try:                                                â”‚\n           â”‚ â± 256 â”‚   â”‚   â”‚   self.client.ping()                              â”‚\n           â”‚   257 â”‚   â”‚   â”‚   log.debug("docker daemon is running and accessi â”‚\n           â”‚   258 â”‚   â”‚   â”‚   checks.append(environmentcheck(                 â”‚\n           â”‚   259 â”‚   â”‚   â”‚   â”‚   name="docker daemon running",               â”‚\n           â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n           attributeerror: \'nonetype\' object has no attribute \'ping\'            \nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ error: failed to create configuration: \'nonetype\' object has no attribute    â”‚\nâ”‚ \'ping\'                                                                       â”‚\nâ”‚                                                                              â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n           installation failed                                                  \n'
  
  'error' is contained here:
    [02:46:41] creating default configuration files in /home/ubuntu/.ollama-stack   
               could not get docker info to check for nvidia runtime.               
               defaulting to cpu platform.                                          
               failed to initialize docker client: error while fetching server api  
  ?                                                +++++
               version: ('connection aborted.', filenotfounderror(2, 'no such file  
               or directory'))                                                      
               starting fresh stack installation...                                 
               initializing fresh stack configuration...                            
               configuration directory already exists: /home/ubuntu/.ollama-stack   
               created configuration directory: /home/ubuntu/.ollama-stack          
               created default configuration files                                  
               configuration files created successfully!                            
               running environment validation checks...                             
               validating environment...                                            
               failed to create configuration: 'nonetype' object has no attribute   
               'ping'                                                               
               â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
               â”‚ /workspace/ollama_stack_cli/stack_manager.py:189 in install_stack â”‚
               â”‚                                                                   â”‚
               â”‚    186 â”‚   â”‚   â”‚   log.info("running environment validation check â”‚
               â”‚    187 â”‚   â”‚   â”‚   log.info("validating environment...")          â”‚
               â”‚    188 â”‚   â”‚   â”‚                                                  â”‚
               â”‚ â±  189 â”‚   â”‚   â”‚   check_report = self.run_environment_checks(fix â”‚
               â”‚    190 â”‚   â”‚   â”‚                                                  â”‚
               â”‚    191 â”‚   â”‚   â”‚   # check if all critical checks passed          â”‚
               â”‚    192 â”‚   â”‚   â”‚   failed_checks = [check for check in check_repo â”‚
               â”‚                                                                   â”‚
               â”‚ /workspace/ollama_stack_cli/stack_manager.py:104 in               â”‚
               â”‚ run_environment_checks                                            â”‚
               â”‚                                                                   â”‚
               â”‚    101 â”‚   â”‚   log.debug("running comprehensive environment check â”‚
               â”‚    102 â”‚   â”‚                                                      â”‚
               â”‚    103 â”‚   â”‚   # delegate environment checks to docker client     â”‚
               â”‚ â±  104 â”‚   â”‚   report = self.docker_client.run_environment_checks â”‚
               â”‚        platform=self.platform)                                    â”‚
               â”‚    105 â”‚   â”‚                                                      â”‚
               â”‚    106 â”‚   â”‚   # add platform-specific checks for native services â”‚
               â”‚    107 â”‚   â”‚   if self.platform == "apple":                       â”‚
               â”‚                                                                   â”‚
               â”‚ /workspace/ollama_stack_cli/docker_client.py:256 in               â”‚
               â”‚ run_environment_checks                                            â”‚
               â”‚                                                                   â”‚
               â”‚   253 â”‚   â”‚                                                       â”‚
               â”‚   254 â”‚   â”‚   # 1. docker daemon check                            â”‚
               â”‚   255 â”‚   â”‚   try:                                                â”‚
               â”‚ â± 256 â”‚   â”‚   â”‚   self.client.ping()                              â”‚
               â”‚   257 â”‚   â”‚   â”‚   log.debug("docker daemon is running and accessi â”‚
               â”‚   258 â”‚   â”‚   â”‚   checks.append(environmentcheck(                 â”‚
               â”‚   259 â”‚   â”‚   â”‚   â”‚   name="docker daemon running",               â”‚
               â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
               attributeerror: 'nonetype' object has no attribute 'ping'            
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ error: failed to create configuration: 'nonetype' object has no attribute    â”‚
    â”‚ 'ping'                                                                       â”‚
    â”‚                                                                              â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
               installation failed
FAILED tests/integration/test_lifecycle_integration.py::test_start_command_without_docker - AssertionError: Expected Docker daemon error message, got: [02:46:43] Could not get Docker info to check for NVIDIA runtime.               
             Defaulting to CPU platform.                                          
             Failed to initialize Docker client: Error while fetching server API  
             version: ('Connection aborted.', FileNotFoundError(2, 'No such file  
             or directory'))                                                      
  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Security Warning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
  â”‚ âš ï¸  Warning: Using default configuration with placeholder security keys   â”‚
  â”‚                                                                          â”‚
  â”‚ ğŸ“‹ Run 'ollama-stack install' to generate a unique, secure configuration â”‚
  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
  
assert False
 +  where False = any(<generator object test_start_command_without_docker.<locals>.<genexpr> at 0x7d8780a8dd80>)
FAILED tests/integration/test_lifecycle_integration.py::test_stop_when_already_stopped_is_idempotent - AssertionError: assert 1 == 0
 +  where 1 = <Result FileNotFoundError(2, 'No such file or directory')>.exit_code
FAILED tests/integration/test_lifecycle_integration.py::test_restart_without_docker - AssertionError: Expected Docker daemon error message, got: [02:46:45] Could not get Docker info to check for NVIDIA runtime.               
             Defaulting to CPU platform.                                          
             Failed to initialize Docker client: Error while fetching server API  
             version: ('Connection aborted.', FileNotFoundError(2, 'No such file  
             or directory'))                                                      
             Restarting Ollama Stack...                                           
             Stopping Docker-based services...                                    
  
assert False
 +  where False = any(<generator object test_restart_without_docker.<locals>.<genexpr> at 0x7d87806d9ff0>)
FAILED tests/integration/test_lifecycle_integration.py::test_check_command_validates_environment - assert 'port' in "[02:46:46] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n           running environment checks...                                        \n"
 +  where "[02:46:46] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n           running environment checks...                                        \n" = <built-in method lower of str object at 0x1d7710c0>()
 +    where <built-in method lower of str object at 0x1d7710c0> = "[02:46:46] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n           Running environment checks...                                        \n".lower
 +      where "[02:46:46] Could not get Docker info to check for NVIDIA runtime.               \n           Defaulting to CPU platform.                                          \n           Failed to initialize Docker client: Error while fetching server API  \n           version: ('Connection aborted.', FileNotFoundError(2, 'No such file  \n           or directory'))                                                      \n           Running environment checks...                                        \n" = <Result AttributeError("'NoneType' object has no attribute 'ping'")>.stdout
FAILED tests/integration/test_migrate_integration.py::test_migrate_validates_migration_prerequisites - assert 2 in [0, 1]
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_creates_backup_before_migration - assert 2 == 0
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_cross_platform_compatibility_validation - assert 2 == 0
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_handles_insufficient_disk_space - assert 2 in [0, 1]
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_handles_corrupted_configuration - assert 2 == 1
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_platform_detection_accuracy - assert 2 == 0
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_after_fresh_installation - assert 2 == 0
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_cleanup_on_failure - assert 2 in [0, 1]
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_migrate_integration.py::test_migrate_output_format_consistency - assert 2 == 0
 +  where 2 = <Result SystemExit(2)>.exit_code
FAILED tests/integration/test_restore_integration.py::test_restore_with_validate_only_flag - assert 'restoring' not in "[02:47:01] could not get docker info to check for nvidia runtime.               \n           defaulting to cpu platform.                                          \n           failed to initialize docker client: error while fetching server api  \n           version: ('connection aborted.', filenotfounderror(2, 'no such file  \n           or directory'))                                                      \n           restoring from backup:                                               \n           /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup\n           validating backup integrity...                                       \n           starting stack restore process...                                    \n           validating backup manifest...                                        \n           backup manifest validation completed successfully                    \n           backup validation passed                                             \n           backup id: test-backup-12345                                         \n           created: 2024-01-01 00:00:00+00:00                                   \n           platform: default                                                    \n           validation-only mode - restore not performed                         \n           backup validation passed                                             \n           validation-only mode - restore not performed                         \nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ validation complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ âœ… backup validation successful                                              â”‚\nâ”‚                                                                              â”‚\nâ”‚ ğŸ“ backup:                                                                   â”‚\nâ”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚\nâ”‚ ğŸ” status: valid and ready for restore                                       â”‚\nâ”‚                                                                              â”‚\nâ”‚ ğŸ’¡ to restore this backup, run:                                              â”‚\nâ”‚    ollama-stack restore                                                      â”‚\nâ”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
  
  'restoring' is contained here:
    [02:47:01] could not get docker info to check for nvidia runtime.               
               defaulting to cpu platform.                                          
               failed to initialize docker client: error while fetching server api  
               version: ('connection aborted.', filenotfounderror(2, 'no such file  
               or directory'))                                                      
               restoring from backup:                                               
  ?            +++++++++
               /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup
               validating backup integrity...                                       
               starting stack restore process...                                    
               validating backup manifest...                                        
               backup manifest validation completed successfully                    
               backup validation passed                                             
               backup id: test-backup-12345                                         
               created: 2024-01-01 00:00:00+00:00                                   
               platform: default                                                    
               validation-only mode - restore not performed                         
               backup validation passed                                             
               validation-only mode - restore not performed                         
    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ validation complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    â”‚ âœ… backup validation successful                                              â”‚
    â”‚                                                                              â”‚
    â”‚ ğŸ“ backup:                                                                   â”‚
    â”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚
    â”‚ ğŸ” status: valid and ready for restore                                       â”‚
    â”‚                                                                              â”‚
    â”‚ ğŸ’¡ to restore this backup, run:                                              â”‚
    â”‚    ollama-stack restore                                                      â”‚
    â”‚ /tmp/pytest-of-ubuntu/pytest-2/test_restore_with_validate_onl0/backup        â”‚
    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
FAILED tests/integration/test_restore_integration.py::test_restore_handles_permission_denied_scenarios - assert False
 +  where False = any(<generator object test_restore_handles_permission_denied_scenarios.<locals>.<genexpr> at 0x7d87806da740>)
FAILED tests/integration/test_restore_integration.py::test_restore_cross_platform_backup_compatibility - FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pytest-of-ubuntu/pytest-2/test_restore_cross_platform_ba0/backup/manifest.json'
FAILED tests/integration/test_restore_integration.py::test_restore_preserves_configuration_files - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/integration/test_restore_integration.py::test_restore_handles_configuration_conflicts - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/integration/test_restore_integration.py::test_restore_output_format_consistency - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/integration/test_uninstall_integration.py::test_uninstall_without_docker_daemon - assert False
 +  where False = any(<generator object test_uninstall_without_docker_daemon.<locals>.<genexpr> at 0x7d8780a1d970>)
FAILED tests/integration/test_update_integration.py::test_update_command_without_docker - assert False
 +  where False = any(<generator object test_update_command_without_docker.<locals>.<genexpr> at 0x7d8780a3d080>)
FAILED tests/integration/test_update_integration.py::test_update_error_message_quality - assert False
 +  where False = any(<generator object test_update_error_message_quality.<locals>.<genexpr> at 0x7d878055d3c0>)
FAILED tests/integration/test_workflow_integration.py::test_user_friendly_error_messages_workflow - assert 2 in [0, 1]
 +  where 2 = <Result SystemExit(2)>.exit_code
============ 25 failed, 40 passed, 105 skipped in 69.34s (0:01:09) =============
